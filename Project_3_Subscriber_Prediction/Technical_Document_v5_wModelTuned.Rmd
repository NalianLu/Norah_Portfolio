---
title: "Homework 2 Technical Document"
author: "Group 6"
date: "`r Sys.Date()`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
gc()
library(dplyr)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(FSelectorRcpp)  # feature selection
library(class)  # feature selection
library(caret)  # train-val-test split
library(rpart)  # decision tree
library(rpart.plot) # decision tree
library(class) # K-NN
#install.packages("ROSE")
library(ROSE)  # over-sampling
library(e1071) # Naive Bayes
library(pROC) # ROC & AUC
data <- read_csv("XYZData.csv")
```

# Table of contents:
1. Problem Scoping
2. Exploratory Data Analysis
3. Predictive Model Training
4. Model Evaluation
5. Model Application
6. Summary

# 1. Problem Scoping
## 1.1 Background Information
Website XYZ, a music-listening social networking website, follows the "freemium" business model. The website offers basic services for free, and provides a number of additional premium capabilities for a monthly subscription fee.

## 1.2 Problem Statement
Know which users would be likely to convert from free users to premium subscribers in the next 6 month period, if they are targeted by the promotional campaign.

## 1.3 Objective
Develop an optimized predictive model that accurately identifies the most probable adopters for the upcoming marketing campaign.

## 1.4 Data Requirement
The user features and results of previous marketing campaigns which targeted a number of non-subscribers

## 1.5 Deliverables
1. A proper performance metric for model evaluation based on the description of the business
2. Sampling techniques to process imbalanced class distribution data
3. The best model that achieves highest performance on the metric of selection after comparing different modeling techniques, different parameter configurations, and feature selection strategies.

# 2. Exploratory Data Analysis
## 2.1 Data Summary
```{r}
# calculate basic statistical measures
summary(data |> select(-user_id))
```
Most of features are positively skewed except for male, avg_friend_male, delta_avg_friend_male, delta_subscriber_friend_cnt, and tenure.

```{r}
# check missing value
sum(is.na(data |> select(-user_id)))
```
There is no row with missing values in the dataset.

```{r}
# check data types of variables
check_data_types <- function(dataset) {
  for (col in names(dataset)) {
    column_data <- dataset[[col]]
    data_type <- class(column_data)  # or typeof(column_data) for fundamental type
    
    # Printing the data type for each column
    cat("Column:", col, "is", data_type, "\n")
  }
}
check_data_types(data |> select(-user_id))
```
We make a copy of the raw data file and change the data type of the following columns to factor for better visualization later. 
```{r}
data_for_visual <- data |> select(-user_id)
data_for_visual$male <- as.factor(data_for_visual$male)
data_for_visual$good_country  <- as.factor(data_for_visual$good_country )
data_for_visual$adopter = as.factor(data_for_visual$adopter)
```


## 2.2 Data Visualization
```{r}
# create various plots to visualize the data distribution, including histograms for numeric variables and bar charts for categorical variables.
visualize_distribution <- function(dataset) {
  # Loop through each column
  for (col in names(dataset)) {
    column_data <- dataset[[col]]
    data_type <- class(column_data)
    
    if (data_type == "numeric") {
      # Create a histogram for numeric columns
      ggplot(data = dataset, aes(x = column_data)) +
        geom_histogram(fill = "skyblue", color = "black") +
        labs(title = paste("Histogram of", col), x = "", y = "")
    } else if (data_type == "character" || data_type == "factor") {
      # Create a bar plot for categorical columns
      ggplot(data = dataset, aes(x = column_data)) +
        geom_bar(fill = "lightgreen", color = "black") +
        labs(title = paste("Bar Plot of", col), x = "", y = "")
    }
    
    # Print the plot
    print(last_plot())
  }
}
visualize_distribution(data_for_visual)
```

```{r}
num_class1 <- data_for_visual |> filter(adopter==1) |> count()
as.numeric(100*num_class1/count(data_for_visual))  # in percentage
```
The adopter distribution is highly imbalanced with only 3.7% are adopters.

## 2.3 Correlation Analysis
```{r}
# calculate correlation coefficients
cor_matrix <- round(cor(data[,2:27]),2)
# visualize correlations using heatmaps
cor_data <- melt(cor_matrix)
ggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "green") +  # Define the color gradient
  labs(title = "Correlation Matrix",
       x = "Variables",
       y = "Variables")
```

```{r}
# Find the top 5 correlated column pairs
top_cor <- findCorrelation(cor_matrix, cutoff = 0.5, names = TRUE)
# Print the top correlated column pairs
print(top_cor)
```
These features have strong correlation (correlation coefficient >= 0.5):

1. age-related
  + cor(age, avg_friend_age) = 0.69
2. friend-related
  + cor(friend_cnt, friend_country_cnt) = 0.69
  + cor(friend_cnt, subscriber_friend_cnt) = 0.65
  + cor(delta_friend_cnt, delta_friend_country_cnt) = 0.65
  + cor(friend_country_cnt, subscriber_friend_cnt) = 0.57
3. track-related
  + cor(lovedTracks, delta_lovedTracks) = 0.51

```{r}
# check correlation with adopter
cor_adopter = cor_matrix[, "adopter"]
sort(cor_adopter, decreasing = TRUE)
```
All features have a low correlation coefficient with adopter.

# 3. Predictive Model Training
## 3.1 Performance Metric
When dealing with highly imbalanced data, accuracy alone may not be a suitable performance metric, as it can be misleading. We considered following classification performance metrics to deal with it.

* *$Precision_{Pos}$*: Precision is particularly important when the positive class is the class of interest (in this case, users becoming subscribers). It measures the proportion of correctly predicted subscribers out of all predicted subscribers. Maximizing precision helps minimize false positives, ensuring that the users predicted as subscribers are more likely to convert into actual subscribers.
* *$Recall_{Pos}$*: Recall becomes crucial when it is important to capture as many true positives (subscribers) as possible. Maximizing recall helps ensure that a higher proportion of actual subscribers are correctly identified by the model.
* *$FMeasure_{Pos}$*: The F1 score is a harmonic mean of precision and recall, providing a balanced measure of the model's performance. It takes into account both precision and recall, making it useful when you want to strike a balance between identifying subscribers accurately (precision) and capturing a good proportion of actual subscribers (recall).

Assuming that promotional campaign is costly, minimizing false positives (predicting non-subscribers as subscribers) is important. Thus, we decide to prioritize $Precision_{Pos}$ metric and make a combination of $Precision_{Pos}$ and $FMeasure_{Pos}$ metrics with different weights to have a comprehensive evaluation of the model's performance. Here is the performance metric we use:

$$
Performance Metric = 0.7*Precision_{Pos} + 0.3*FMeasure_{Pos}
$$

## 3.2 Oversampling on Training Data
We use ROSE (Random Over-Sampling Examples) package for oversampling.
```{r}
data$adopter <- as.factor(data$adopter)
# split the dataset into 70% training, 20% validation, and 10% testing
set.seed(123)  # set the seed for reproducibility
train_rows <- createDataPartition(y = data$adopter, p = 0.7, list = FALSE)
data_train <- data[train_rows,] |> select(-user_id)
data_remain <- data[-train_rows,] |> select(-user_id)
validation_rows <- createDataPartition(y = data_remain$adopter, p = 0.67, list = FALSE)
data_validation <- data_remain[validation_rows,]
data_test <- data_remain[-validation_rows,]
```
Remove user_id column because it is not a feature.

```{r}
# oversampling on training data for 3 different scenarios
balanced_data_train_30 <- ovun.sample(adopter ~ ., data = data_train, method = "over", p = 0.3, seed = 123)$data
balanced_data_train_20 <- ovun.sample(adopter ~ ., data = data_train, method = "over", p = 0.2, seed = 123)$data
balanced_data_train_10 <- ovun.sample(adopter ~ ., data = data_train, method = "over", p = 0.1, seed = 123)$data
```
The `ovun.sample()` function creates possibly balanced samples by random over-sampling minority observations. In this case, it takes the formula adopter ~ ., where adopter is our target variable, and . represents all other predictor variables. The method parameter is set to "over" to perform oversampling. The P parameter is set to the probability of resampling from the rare class. Here we set p equals 10%, 20%, and 30% respectively to compare 3 different scenarios. The seed parameter is used for reproducibility.

```{r}
as.numeric((balanced_data_train_30 |> filter(adopter==1) |> count())/count(balanced_data_train_30))  # in percentage
as.numeric((balanced_data_train_20 |> filter(adopter==1) |> count())/count(balanced_data_train_20))  # in percentage
as.numeric((balanced_data_train_10 |> filter(adopter==1) |> count())/count(balanced_data_train_10))  # in percentage
```
The adopters in the training data account for roughly 30%, 20%, and 10% now. We will build our prediction models based on these balanced training data.

## 3.3 K-NN Prediction Model
Define a function that trains a K-NN model using the current set of features and returns the performance metric we defined.
```{r}
# Define the K-NN Model Function
knn_cv <- function(features, target, test_features, k) {
  pred_knn <- knn(train = features,
                  test = test_features,
                  cl = target,
                  k = k)
  conf_matrix = confusionMatrix(data = pred_knn, 
                                reference = data_validation_norm$adopter,
                                mode = "prec_recall",
                                positive = '1')
  precision_knn <- conf_matrix$byClass["Pos Pred Value"]
  f1_knn <- conf_matrix$byClass["F1"]
  return(0.7*precision_knn+0.3*f1_knn)
}
```
We will use normalized data to calculate distance and train K-NN prediction models.
```{r}
# normalize
normalize = function(x){
  return ((x - min(x))/(max(x) - min(x)))
  }
```

### 3.3.1 Oversampling p = 0.3
```{r}
balanced_data_train_norm <- balanced_data_train_30 |> mutate_at(1:25, normalize)
data_validation_norm = data_validation |> mutate_at(1:25, normalize)
# Prepare the Data
target_column_index = 26
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
```
We try k values from 2 to 20 and evaluate its performance respectively.
```{r}
# train a K-NN model for a few k values
for (kval in 2:20) {
  performance_knn <- knn_cv(X,Y,data_validation_norm[, -target_column_index],kval)
  cat(sprintf("Performance with k = %d is: %f \n", kval, performance_knn))
}
```
The 30% oversampling model performs best when k = 4 as 0.063160. Performance hovers around 0.0 after k = 4 and then comes below 0.06.

### 3.3.2 Oversampling p = 0.2
```{r}
balanced_data_train_norm <- balanced_data_train_20 |> mutate_at(1:25, normalize)
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
# train a K-NN model for a few k values
for (kval in 2:20) {
  performance_knn <- knn_cv(X,Y,data_validation_norm[, -target_column_index],kval)
  cat(sprintf("Performance with k = %d is: %f \n", kval, performance_knn))
}
```
The 20% oversampling model performs best when k = 4 as 0.062685. Performance decreases gradually after k =4 and then goes back to 0.058.

### 3.3.3 Oversampling p = 0.1
```{r}
balanced_data_train_norm <- balanced_data_train_10 |> mutate_at(1:25, normalize)
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
# train a K-NN model for a few k values
for (kval in 2:20) {
  performance_knn <- knn_cv(X,Y,data_validation_norm[, -target_column_index],kval)
  cat(sprintf("Performance with k = %d is: %f \n", kval, performance_knn))
}
```
The 10% oversampling model performs best when k = 3 as 0.067485. Performance is below 0.06 after k = 4.

### 3.3.4 K-NN Feature Selection
The K-NN model with 10% oversampling and k = 3 has the best performance as of 0.067485. We then perform backward feature selection on it to select the best subset of features to avoid overfitting.
```{r}
# use data with 10% oversampling
balanced_data_train_norm <- balanced_data_train_10 |> mutate_at(1:25, normalize)
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
```
Define backward selection function.
```{r}
backward_selection_knn <- function(X, Y, k) {
  n_features <- ncol(X)
  selected_features <- seq_len(n_features)  # Initialize with all features
  
  for (i in seq_len(n_features)) {
    best_performance <- 0
    worst_feature <- NULL
    
    # Iterate through all remaining features to find the worst one to remove
    for (j in selected_features) {
      current_features <- setdiff(selected_features, j)
      performance <- knn_cv(X[, current_features], Y, data_validation_norm[, current_features], k)
      
      if (!is.na(performance) && performance > best_performance) {  # Check for missing values in accuracy
        best_performance <- performance
        worst_feature <- j
      }
    }
    
    # Remove the worst feature from the selected features
    selected_features <- setdiff(selected_features, worst_feature)
    
    # Print the progress (optional)
    cat("Removed feature", names(X)[worst_feature], "with performance:", best_performance, "\n")
  }
  
  return(selected_features)
}
```
Call the function with k = 3
```{r}
backward_selection_knn(X,Y,3)
```
The model performs best when "removed feature playlists" shows with performance as of 0.09666835. The error occurs because after removing feature delta_shouts, all the distances are the same and the model cannot compare further.

### 3.3.5 Fine-Tuned K-NN
Train K-NN model with selected-feature, 10% oversampling, and K=3.
```{r}
# get selected features
all_features <- colnames(balanced_data_train_10) 
elements_to_remove <- c("delta_good_country","friend_cnt", "posts", "delta_playlists", "delta_friend_country_cnt","delta_friend_cnt", "good_country", "avg_friend_age", "delta_posts", "songsListened","shouts","playlists")
selected_features_knn = setdiff(all_features, elements_to_remove)
# prepare training data
balanced_data_train_norm <- balanced_data_train_10 |> mutate_at(1:25, normalize)
balanced_data_train_norm <- balanced_data_train_norm[selected_features_knn]  # 14 columns
data_validation_norm <- data_validation_norm[selected_features_knn]
# train K-NN model with selected features 
knn_fine_tuned = knn(train = balanced_data_train_norm[,1:13],
                 test = data_validation_norm[,1:13],
                 cl = balanced_data_train_norm[,14],
                 k = 3)
# double-check model performance on validation data
# knn_cv(balanced_data_train_norm[,1:13],balanced_data_train_norm[,14],data_validation_norm[,1:13],3)
```

## 3.4 Decision Tree Prediction Model
We use rpart package to build the Decision Tree.
### 3.4.1 Oversampling p = 0.3
```{r}
# train a decision tree model
tree_30 = rpart(adopter ~ ., data = balanced_data_train_30,
             method = "class",
             parms = list(split = "information"))
```
The `rpart()` function does recursive partition. It takes the formula adopter ~ ., where adopter is our target variable, and . represents all other predictor variables. The method parameter is set to "class" to specify that the task is classification. The parms parameter allows us to specify other parameters, such as splitting criterion. Here we set split = "information" meaning that we want to determine splits based on information gain.
```{r}
# make prediction
pred_tree = predict(tree_30, data_validation, type = "class")
# get the performance
conf_matrix = confusionMatrix(data = pred_tree,
                              reference = data_validation$adopter,
                              mode = "prec_recall",
                              positive = '1')

# evaluate performance
precision_dt <- conf_matrix$byClass["Pos Pred Value"]
f1_dt <- conf_matrix$byClass["F1"]
performance_dt <- setNames(0.7*precision_dt+0.3*f1_dt,'performance_dt')
print(performance_dt)
```
The Decision Tree built with 30% oversampling data has a performance metric value of 0.150255

### 3.4.2 Oversampling p = 0.2
```{r}
# train a decision tree model
tree_20 = rpart(adopter ~ ., data = balanced_data_train_20,
             method = "class",
             parms = list(split = "information"))
```
The `rpart()` function does recursive partition. It takes the formula adopter ~ ., where adopter is our target variable, and . represents all other predictor variables. The method parameter is set to "class" to specify that the task is classification. The parms parameter allows us to specify other parameters, such as splitting criterion. Here we set split = "information" meaning that we want to determine splits based on information gain.
```{r}
# make prediction
pred_tree = predict(tree_20, data_validation, type = "class")
# get the performance
conf_matrix = confusionMatrix(data = pred_tree,
                              reference = data_validation$adopter,
                              mode = "prec_recall",
                              positive = '1')

# evaluate performance
precision_dt <- conf_matrix$byClass["Pos Pred Value"]
f1_dt <- conf_matrix$byClass["F1"]
performance_dt <- setNames(0.7*precision_dt+0.3*f1_dt,'performance_dt')
print(performance_dt)
```
The Decision Tree built with 20% oversampling data has a performance metric value of 0.1643227, which is higher than 30% oversampling one.

### 3.4.3 Oversampling p = 0.1
```{r}
# train a decision tree model
tree_10 = rpart(adopter ~ ., data = balanced_data_train_10,
             method = "class",
             parms = list(split = "information"))
```
The `rpart()` function does recursive partition. It takes the formula adopter ~ ., where adopter is our target variable, and . represents all other predictor variables. The method parameter is set to "class" to specify that the task is classification. The parms parameter allows us to specify other parameters, such as splitting criterion. Here we set split = "information" meaning that we want to determine splits based on information gain.
```{r}
# make prediction
pred_tree = predict(tree_10, data_validation, type = "class")
# get the performance
conf_matrix = confusionMatrix(data = pred_tree,
                              reference = data_validation$adopter,
                              mode = "prec_recall",
                              positive = '1')

# evaluate performance
precision_dt <- conf_matrix$byClass["Pos Pred Value"]
f1_dt <- conf_matrix$byClass["F1"]
performance_dt <- setNames(0.7*precision_dt+0.3*f1_dt,'performance_dt')
print(performance_dt)
```
The Decision Tree built with 10% oversampling data has a performance metric value of NaN. The model predicts no rows as class 1, which results in 0 denominator for the precision_pos and FMeasure metrics.

### 3.4.4 Fine-Tune Decision Tree
The 20% oversampling Decision Tree model performs best as of 0.1643227. We fine-tune it using tree pruning and then

```{r}
# visualize the current decision tree
prp(tree_20, varlen = 0)
```
We try different minimum information gain requirement for a split.
```{r}
for (ig in seq(0.005,0.015,0.001)) {
  tree = rpart(adopter ~ ., data = balanced_data_train_20,
               method = "class",
               parms = list(split = "information"),
               control = list(cp = ig))
  pred_tree = predict(tree, data_validation, type = "class")
  conf_matrix = confusionMatrix(data = pred_tree,
                                reference = data_validation$adopter,
                                mode = "prec_recall",
                                positive = '1')
  precision_dt <- conf_matrix$byClass["Pos Pred Value"]
  f1_dt <- conf_matrix$byClass["F1"]
  performance_dt <- setNames(0.7*precision_dt+0.3*f1_dt,'performance_dt')
  cat("for cp = ", ig, ", the performance is: ", performance_dt, "\n")
}
```
cp (minimum information gain requirement for a split) doesn't change model performance a lot. We use default cp value.

We then try different minimum number of observations that must exist in a node in order for a split to be attempted.

```{r}
for (ms in 1:10) {
  tree = rpart(adopter ~ ., data = balanced_data_train_20,
               method = "class",
               parms = list(split = "information"),
               control = list(minsplit = ms))
  pred_tree = predict(tree, data_validation, type = "class")
  conf_matrix = confusionMatrix(data = pred_tree,
                                reference = data_validation$adopter,
                                mode = "prec_recall",
                                positive = '1')
  precision_dt <- conf_matrix$byClass["Pos Pred Value"]
  f1_dt <- conf_matrix$byClass["F1"]
  performance_dt <- setNames(0.7*precision_dt+0.3*f1_dt,'performance_dt')
  cat("for ms = ", ms, ", the performance is: ", performance_dt, "\n")
}
```
minsplit (minimum information gain requirement for a split) doesn't change model performance a lot. We use default minsplit value.

We then try different maximum depth of any node of the final tree.
```{r}
for (md in 1:10) {
  tree = rpart(adopter ~ ., data = balanced_data_train_20,
               method = "class",
               parms = list(split = "information"),
               control = list(maxdepth = md))
  pred_tree = predict(tree, data_validation, type = "class")
  conf_matrix = confusionMatrix(data = pred_tree,
                                reference = data_validation$adopter,
                                mode = "prec_recall",
                                positive = '1')
  precision_dt <- conf_matrix$byClass["Pos Pred Value"]
  f1_dt <- conf_matrix$byClass["F1"]
  performance_dt <- setNames(0.7*precision_dt+0.3*f1_dt,'performance_dt')
  cat("for md = ", md, ", the performance is: ", performance_dt, "\n")
}
```
The performance for current decision is 0.1643227, which is equal to or larger than fine-tuning results.

## 3.5 Naive Bayes Prediction Model
Define a function that trains a Naive Bayes model using the current set of features and returns the performance metric we defined.
```{r}
nb_cv <- function(data_train, data_validation) {
  # build a naive Bayes model
  nb_model = naiveBayes(adopter ~ ., data = data_train)
  # make prediction
  pred_nb = predict(nb_model, data_validation)
  # get the performance
  conf_matrix = confusionMatrix(data = pred_nb,
                                reference = data_validation$adopter,
                                mode = "prec_recall",
                                positive = '1')
  # evaluate performance
  precision_nb <- conf_matrix$byClass["Pos Pred Value"]
  f1_nb <- conf_matrix$byClass["F1"]
  performance_nb <- setNames(0.7*precision_nb+0.3*f1_nb,'performance_nb')
  return(performance_nb)
}
```

### 3.5.1 Oversampling p = 0.3
Call nb_cv functions on 30% oversampling data
```{r}
nb_cv(balanced_data_train_30, data_validation)
```
The training data with 30% oversampling has a performance as of 0.1311784.

### 3.5.2 Oversampling p = 0.2
Call nb_cv functions on 20% oversampling data
```{r}
nb_cv(balanced_data_train_20, data_validation)
```
The training data with 20% oversampling has a performance as of 0.1324612.

### 3.5.3 Oversampling p = 0.1
Call nb_cv functions on 10% oversampling data
```{r}
nb_cv(balanced_data_train_10, data_validation)
```
The training data with 10% oversampling has a performance as of 0.139847.

### 3.5.4 Naive Bayes Feature Selection
The Naive Bayes model with 10% oversampling has the best performance as of 0.139847. We then perform backward feature selection on it to select the best subset of features to avoid overfitting.
```{r}
# use data with 10% oversampling
X = balanced_data_train_10[,1:25]  # features
Y = balanced_data_train_10[,26]  # target variable
A = data_validation[,1:25]  # validation data features
B = data_validation[,26]  # validation data target variable
```
Define backward selection function.
```{r}
backward_selection_nb <- function(X,Y,A,B) {
  n_features <- ncol(X)
  selected_features <- seq_len(n_features)  # Initialize with all features
  
  for (i in seq_len(n_features)) {
    best_performance <- 0
    worst_feature <- NULL
    
    # Iterate through all remaining features to find the worst one to remove
    for (j in selected_features) {
      current_features <- setdiff(selected_features, j)
      
      data_train = cbind(X[current_features],Y)
      # change name
      column_names <- names(data_train)
      column_names[column_names == 'Y'] <- "adopter"
      names(data_train) <- column_names
      
      data_validation = cbind(A[current_features],B)
      
      performance <- nb_cv(data_train,data_validation)
      
      if (!is.na(performance) && performance > best_performance) {  # Check for missing values in accuracy
        best_performance <- performance
        worst_feature <- j
      }
    }
    
    # Remove the worst feature from the selected features
    selected_features <- setdiff(selected_features, worst_feature)
    
    # Print the progress (optional)
    cat("Removed feature", names(X)[worst_feature], "with performance:", best_performance, "\n")
  }
  
  return(selected_features)
}
```
Call backward selection function.
```{r}
backward_selection_nb(X,Y,A,B)
```
Based on feature selection results, the Naive Bayes model works best when it only uses subscriber_friend_cnt feature with a performance of 0.1963899. The error occurs because there should be at least one column as feature.

### 3.3.5 Fine-Tuned Naive Bayes
Train Naive Bayes model only using selected-feature subscriber_friend_cnt, and 10% oversampling.
```{r}
# prepare training data
data_train_nb = balanced_data_train_10[c("subscriber_friend_cnt","adopter")]
# train K-NN model with 
nb_fine_tuned = naiveBayes(adopter ~ ., data = data_train_nb)

# double-check model performance on validation data
nb_cv(cbind(balanced_data_train_10["subscriber_friend_cnt"],balanced_data_train_10["adopter"]), cbind(data_validation["subscriber_friend_cnt"],data_validation["adopter"]))
```

# 4. Model Evaluation
Compare 3 different models using the performance metric we defined and conduct cross validation on each of them to compare its robustness.
## 4.1 Performance Comparision

## 4.2 Cross Validation
...
# 5. Model Application
We choose XXX as the best predictive model and use it on testing data to understand how it perform.
## 5.1 Performance Metric
```{r}
# make prediction
pred = predict(nb_model, data_test)
# get the performance
conf_matrix = confusionMatrix(data = pred,
                              reference = data_test$adopter,
                              mode = "prec_recall",
                              positive = '1')
# evaluate performance
precision <- conf_matrix$byClass["Pos Pred Value"]
f1 <- conf_matrix$byClass["F1"]
performance <- setNames(0.7*precision+0.3*f1,'performance')
print(performance)
```
## 5.2 ROC & AUC
We also make a ROC Curve and then calculate AUC metric for it.
```{r}
prob_pred = predict(nb_model, data_test, type = "raw")
data_test_roc = data_test %>%
  mutate(prob = prob_pred[,"1"]) %>%
  arrange(desc(prob)) %>%
  mutate(adopter_yes = ifelse(adopter==1,1,0)) %>%
  mutate(TPR = cumsum(adopter_yes)/sum(adopter_yes),
         FPR = cumsum(1-adopter_yes)/sum(1-adopter_yes))
ggplot(data = data_test_roc, aes(x = FPR, y = TPR)) +
  geom_line() +
  theme_bw()
```
```{r}
roc = roc(response = data_test_roc$adopter_yes,
          predictor = data_test_roc$prob)
auc(roc)
```
AUC between 0.7 and 0.9 represents a good classifier, with reasonably strong discriminative ability and predictive power.

## 5.3 Lift Curve
```{r}
data_test_lift = data_test %>%
  mutate(prob = prob_pred[,"1"]) %>%
  arrange(desc(prob)) %>%
  mutate(adopter_yes = ifelse(adopter==1,1,0)) %>%
  mutate(x = row_number()/nrow(data_test),
         y = (cumsum(adopter_yes)/sum(adopter_yes))/x)
ggplot(data = data_test_lift, aes(x = x, y = y)) +
  geom_line() +
  theme_bw()
```



# 6. Summary
...


Further Work We Can Do: 

1. more explaination about `ovun.sample()` function. How does it work? Does it generate using copy paste or artifically creating or others?
2. more EDA + more visualization (group_by adopter);
3. ROC,AUC for 3 models -> validate the choice of model based on our performance metric; 
4. clustering + feature selections -> modify K-NN and Naive Bayes;
5. weighted distance -> modify K-NN;
6. tree Pruning -> modify Decision Tree;
7. cross validation for 3 models -> validate the robustness of our choosed model;
8. prediction performance visualization