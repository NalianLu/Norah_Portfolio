---
title: "Homework 2 Technical Document"
author: "Group 6"
date: "`r Sys.Date()`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
gc()
library(dplyr)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(FSelectorRcpp)  # feature selection
library(class)  # feature selection
library(caret)  # train-val-test split
library(rpart)  # decision tree
library(rpart.plot) # decision tree
library(class) # K-NN
#install.packages("ROSE")
library(ROSE)  # over-sampling
library(e1071) # Naive Bayes
library(pROC) # ROC & AUC
data <- read_csv("XYZData.csv")
```

# Table of contents:
1. Problem Scoping
2. Exploratory Data Analysis
3. Predictive Model Training
4. Model Evaluation
5. Model Application
6. Summary

# 1. Problem Scoping
## 1.1 Background Information
Website XYZ, a music-listening social networking website, follows the "freemium" business model. The website offers basic services for free, and provides a number of additional premium capabilities for a monthly subscription fee.

## 1.2 Problem Statement
Know which users would be likely to convert from free users to premium subscribers in the next 6 month period, if they are targeted by the promotional campaign.

## 1.3 Objective
Develop an optimized predictive model that accurately identifies the most probable adopters for the upcoming marketing campaign.

## 1.4 Data Requirement
The user features and results of previous marketing campaigns which targeted a number of non-subscribers

## 1.5 Deliverables
1. A proper performance metric for model evaluation based on the description of the business
2. Sampling techniques to process imbalanced class distribution data
3. The best model that achieves highest performance on the metric of selection after comparing different modeling techniques, different parameter configurations, and feature selection strategies.

# 2. Exploratory Data Analysis
## 2.1 Data Summary
```{r}
# calculate basic statistical measures
summary(data |> select(-user_id))
```
Most of features are positively skewed except for male, avg_friend_male, delta_avg_friend_male, delta_subscriber_friend_cnt, and tenure.

```{r}
# check missing value
sum(is.na(data |> select(-user_id)))
```
There is no row with missing values in the dataset.

```{r}
# check data types of variables
check_data_types <- function(dataset) {
  for (col in names(dataset)) {
    column_data <- dataset[[col]]
    data_type <- class(column_data)  # or typeof(column_data) for fundamental type
    
    # Printing the data type for each column
    cat("Column:", col, "is", data_type, "\n")
  }
}
check_data_types(data |> select(-user_id))
```
We make a copy of the raw data file and change the data type of the following columns to factor for better visualization later. 
```{r}
data_for_visual <- data |> select(-user_id)
data_for_visual$male <- as.factor(data_for_visual$male)
data_for_visual$good_country  <- as.factor(data_for_visual$good_country )
data_for_visual$adopter = as.factor(data_for_visual$adopter)
```


## 2.2 Data Visualization
```{r}
# create various plots to visualize the data distribution, including histograms for numeric variables and bar charts for categorical variables.
visualize_distribution <- function(dataset) {
  # Loop through each column
  for (col in names(dataset)) {
    column_data <- dataset[[col]]
    data_type <- class(column_data)
    
    if (data_type == "numeric") {
      # Create a histogram for numeric columns
      ggplot(data = dataset, aes(x = column_data)) +
        geom_histogram(fill = "skyblue", color = "black") +
        labs(title = paste("Histogram of", col), x = "", y = "")
    } else if (data_type == "character" || data_type == "factor") {
      # Create a bar plot for categorical columns
      ggplot(data = dataset, aes(x = column_data)) +
        geom_bar(fill = "lightgreen", color = "black") +
        labs(title = paste("Bar Plot of", col), x = "", y = "")
    }
    
    # Print the plot
    print(last_plot())
  }
}
visualize_distribution(data_for_visual)
```

```{r}
num_class1 <- data_for_visual |> filter(adopter==1) |> count()
as.numeric(100*num_class1/count(data_for_visual))  # in percentage
```
The adopter distribution is highly imbalanced with only 3.7% are adopters.

## 2.3 Correlation Analysis
```{r}
# calculate correlation coefficients
cor_matrix <- round(cor(data[,2:27]),2)
# visualize correlations using heatmaps
cor_data <- melt(cor_matrix)
ggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "green") +  # Define the color gradient
  labs(title = "Correlation Matrix",
       x = "Variables",
       y = "Variables")
```

```{r}
# Find the top 5 correlated column pairs
top_cor <- findCorrelation(cor_matrix, cutoff = 0.5, names = TRUE)
# Print the top correlated column pairs
print(top_cor)
```
These features have strong correlation (correlation coefficient >= 0.5):

1. age-related
  + cor(age, avg_friend_age) = 0.69
2. friend-related
  + cor(friend_cnt, friend_country_cnt) = 0.69
  + cor(friend_cnt, subscriber_friend_cnt) = 0.65
  + cor(delta_friend_cnt, delta_friend_country_cnt) = 0.65
  + cor(friend_country_cnt, subscriber_friend_cnt) = 0.57
3. track-related
  + cor(lovedTracks, delta_lovedTracks) = 0.51

```{r}
# check correlation with adopter
cor_adopter = cor_matrix[, "adopter"]
sort(cor_adopter, decreasing = TRUE)
```
All features have a low correlation coefficient with adopter.

# 3. Predictive Model Training
## 3.1 Performance Metric
When dealing with highly imbalanced data, accuracy alone may not be a suitable performance metric, as it can be misleading. We considered following classification performance metrics to deal with it.

* *$Precision_{Pos}$*: Precision is particularly important when the positive class is the class of interest (in this case, users becoming subscribers). It measures the proportion of correctly predicted subscribers out of all predicted subscribers. Maximizing precision helps minimize false positives, ensuring that the users predicted as subscribers are more likely to convert into actual subscribers.
* *$Recall_{Pos}$*: Recall becomes crucial when it is important to capture as many true positives (subscribers) as possible. Maximizing recall helps ensure that a higher proportion of actual subscribers are correctly identified by the model.
* *$FMeasure_{Pos}$*: The F1 score is a harmonic mean of precision and recall, providing a balanced measure of the model's performance. It takes into account both precision and recall, making it useful when you want to strike a balance between identifying subscribers accurately (precision) and capturing a good proportion of actual subscribers (recall).

Assuming that promotional campaign is costly, minimizing false positives (predicting non-subscribers as subscribers) is important. Thus, we decide to prioritize $Precision_{Pos}$ metric and make a combination of $Precision_{Pos}$ and $FMeasure_{Pos}$ metrics with different weights to have a comprehensive evaluation of the model's performance. Here is the performance metric we use:

$$
Performance Metric = 0.7*Precision_{Pos} + 0.3*FMeasure_{Pos}
$$

## 3.2 Oversampling on Training Data
We use ROSE (Random Over-Sampling Examples) package for oversampling.
```{r}
data$adopter <- as.factor(data$adopter)
# split the dataset into 70% training, 20% validation, and 10% testing
set.seed(123)  # set the seed for reproducibility
train_rows <- createDataPartition(y = data$adopter, p = 0.7, list = FALSE)
data_train <- data[train_rows,] |> select(-user_id)
data_remain <- data[-train_rows,] |> select(-user_id)
validation_rows <- createDataPartition(y = data_remain$adopter, p = 0.67, list = FALSE)
data_validation <- data_remain[validation_rows,]
data_test <- data_remain[-validation_rows,]
```
Remove user_id column because it is not a feature.

```{r}
# oversampling on training data for 3 different scenarios
balanced_data_train_30 <- ovun.sample(adopter ~ ., data = data_train, method = "over", p = 0.3, seed = 123)$data
balanced_data_train_20 <- ovun.sample(adopter ~ ., data = data_train, method = "over", p = 0.2, seed = 123)$data
balanced_data_train_10 <- ovun.sample(adopter ~ ., data = data_train, method = "over", p = 0.1, seed = 123)$data
```
The `ovun.sample()` function creates possibly balanced samples by random over-sampling minority observations. In this case, it takes the formula adopter ~ ., where adopter is our target variable, and . represents all other predictor variables. The method parameter is set to "over" to perform oversampling. The P parameter is set to the probability of resampling from the rare class. Here we set p equals 10%, 20%, and 30% respectively to compare 3 different scenarios. The seed parameter is used for reproducibility.

```{r}
as.numeric((balanced_data_train_30 |> filter(adopter==1) |> count())/count(balanced_data_train_30))  # in percentage
as.numeric((balanced_data_train_20 |> filter(adopter==1) |> count())/count(balanced_data_train_20))  # in percentage
as.numeric((balanced_data_train_10 |> filter(adopter==1) |> count())/count(balanced_data_train_10))  # in percentage
```
The adopters in the training data account for roughly 30%, 20%, and 10% now. We will build our prediction models based on these balanced training data.

## 3.3 K-NN Prediction Model
Define a function that trains a K-NN model using the current set of features and returns the performance metric we defined. We will use cross-validation to get a more reliable estimate of the model's performance.
```{r}
# Define the K-NN Model Function
knn_cv <- function(features, target, test_features, k) {
  pred_knn <- knn(train = features,
                  test = test_features,
                  cl = target,
                  k = k)
  conf_matrix = confusionMatrix(data = pred_knn, 
                                reference = data_validation_norm$adopter,
                                mode = "prec_recall",
                                positive = '1')
  precision_knn <- conf_matrix$byClass["Pos Pred Value"]
  f1_knn <- conf_matrix$byClass["F1"]
  return(0.7*precision_knn+0.3*f1_knn)
}
```
We will use normalized data to calculate distance and train K-NN prediction models.
```{r}
# normalize
normalize = function(x){
  return ((x - min(x))/(max(x) - min(x)))
  }
```

### 3.3.1 Oversampling p = 0.3
```{r}
balanced_data_train_norm <- balanced_data_train_30 |> mutate_at(1:25, normalize)
data_validation_norm = data_validation |> mutate_at(1:25, normalize)
# Prepare the Data
target_column_index = 26
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
```
We try k values from 2 to 20 and evaluate its performance respectively.
```{r}
# train a K-NN model for a few k values
for (kval in 2:20) {
  performance_knn <- knn_cv(X,Y,data_validation_norm[, -target_column_index],kval)
  cat(sprintf("Performance with k = %d is: %f \n", kval, performance_knn))
}
```
The 30% oversampling model performs best when k = 4 as 0.063160. Performance hovers around 0.0 after k = 4 and then comes below 0.06.

### 3.3.2 Oversampling p = 0.2
```{r}
balanced_data_train_norm <- balanced_data_train_20 |> mutate_at(1:25, normalize)
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
# train a K-NN model for a few k values
for (kval in 2:20) {
  performance_knn <- knn_cv(X,Y,data_validation_norm[, -target_column_index],kval)
  cat(sprintf("Performance with k = %d is: %f \n", kval, performance_knn))
}
```
The 20% oversampling model performs best when k = 4 as 0.062685. Performance decreases gradually after k =4 and then goes back to 0.058.

### 3.3.3 Oversampling p = 0.1
```{r}
balanced_data_train_norm <- balanced_data_train_10 |> mutate_at(1:25, normalize)
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
# train a K-NN model for a few k values
for (kval in 2:20) {
  performance_knn <- knn_cv(X,Y,data_validation_norm[, -target_column_index],kval)
  cat(sprintf("Performance with k = %d is: %f \n", kval, performance_knn))
}
```
The 10% oversampling model performs best when k = 3 as 0.067485. Performance is below 0.06 after k = 4.

### 3.3.4 Feature Selection
The K-NN model with 10% oversampling and k = 3 has the best performance as of 0.067485. We then perform backward feature selection on it to select the best subset of features to avoid overfitting.
```{r}
# use data with 10% oversampling
balanced_data_train_norm <- balanced_data_train_10 |> mutate_at(1:25, normalize)
X <- balanced_data_train_norm[, -target_column_index]  # Features
Y <- balanced_data_train_norm[, target_column_index]  # Target variable
```
Define backward selection function.
```{r}
backward_selection_knn <- function(X, Y, k) {
  n_features <- ncol(X)
  selected_features <- seq_len(n_features)  # Initialize with all features
  
  for (i in seq_len(n_features)) {
    best_performance <- 0
    worst_feature <- NULL
    
    # Iterate through all remaining features to find the worst one to remove
    for (j in selected_features) {
      current_features <- setdiff(selected_features, j)
      performance <- knn_cv(X[, current_features], Y, data_validation_norm[, current_features], k)
      
      if (!is.na(performance) && performance > best_performance) {  # Check for missing values in accuracy
        best_performance <- performance
        worst_feature <- j
      }
    }
    
    # Remove the worst feature from the selected features
    selected_features <- setdiff(selected_features, worst_feature)
    
    # Print the progress (optional)
    cat("Removed feature", names(X)[worst_feature], "with performance:", best_performance, "\n")
  }
  
  return(selected_features)
}
```
Call the function with k = 3
```{r}
selected_features = backward_selection_knn(X,Y,3)
```

## 3.4 Decision Tree Prediction Model
We use rpart package to build the Decision Tree.
```{r}
# train a decision tree model
tree = rpart(adopter ~ ., data = balanced_data_train,
             method = "class",
             parms = list(split = "information"))
```
The `rpart()` function does recursive partition. It takes the formula adopter ~ ., where adopter is our target variable, and . represents all other predictor variables. The method parameter is set to "class" to specify that the task is classification. The parms parameter allows us to specify other parameters, such as splitting criterion. Here we set split = "information" meaning that we want to determine splits based on information gain.

```{r}
# visualize the decision tree
prp(tree, varlen = 0)
```


```{r}
# make prediction
pred_tree = predict(tree, data_validation, type = "class")
# get the performance
conf_matrix = confusionMatrix(data = pred_tree,
                              reference = data_validation$adopter,
                              mode = "prec_recall",
                              positive = '1')

# evaluate performance
precision_dt <- conf_matrix$byClass["Pos Pred Value"]
f1_dt <- conf_matrix$byClass["F1"]
performance_dt <- setNames(0.7*precision_dt+0.3*f1_dt,'performance_dt')
print(c(performance_dt, precision_dt, f1_dt))
```

## 3.5 Naive Bayes Prediction Model
We use the naiveBayes function in the e1071 package.
```{r}
# build a naive Bayes model
nb_model = naiveBayes(adopter ~ ., data = balanced_data_train)
# make prediction
pred_nb = predict(nb_model, data_validation)
# get the performance
conf_matrix = confusionMatrix(data = pred_nb,
                              reference = data_validation$adopter,
                              mode = "prec_recall",
                              positive = '1')
# evaluate performance
precision_nb <- conf_matrix$byClass["Pos Pred Value"]
f1_nb <- conf_matrix$byClass["F1"]
performance_nb <- setNames(0.7*precision_nb+0.3*f1_nb,'performance_nb')
print(c(performance_nb, precision_nb, f1_nb))
```

# 4. Model Evaluation
## 4.1 Performance Comparision
... 
## 4.2 Cross Validation
...
# 5. Model Application
We choose XXX as the best predictive model and use it on testing data to understand how it perform.
## 5.1 Performance Metric
```{r}
# make prediction
pred = predict(nb_model, data_test)
# get the performance
conf_matrix = confusionMatrix(data = pred,
                              reference = data_test$adopter,
                              mode = "prec_recall",
                              positive = '1')
# evaluate performance
precision <- conf_matrix$byClass["Pos Pred Value"]
f1 <- conf_matrix$byClass["F1"]
performance <- setNames(0.7*precision+0.3*f1,'performance')
print(performance)
```
## 5.2 ROC & AUC
We also make a ROC Curve and then calculate AUC metric for it.
```{r}
prob_pred = predict(nb_model, data_test, type = "raw")
data_test_roc = data_test %>%
  mutate(prob = prob_pred[,"1"]) %>%
  arrange(desc(prob)) %>%
  mutate(adopter_yes = ifelse(adopter==1,1,0)) %>%
  mutate(TPR = cumsum(adopter_yes)/sum(adopter_yes),
         FPR = cumsum(1-adopter_yes)/sum(1-adopter_yes))
ggplot(data = data_test_roc, aes(x = FPR, y = TPR)) +
  geom_line() +
  theme_bw()
```
```{r}
roc = roc(response = data_test_roc$adopter_yes,
          predictor = data_test_roc$prob)
auc(roc)
```
AUC between 0.7 and 0.9 represents a good classifier, with reasonably strong discriminative ability and predictive power.

## 5.3 Lift Curve
```{r}
data_test_lift = data_test %>%
  mutate(prob = prob_pred[,"1"]) %>%
  arrange(desc(prob)) %>%
  mutate(adopter_yes = ifelse(adopter==1,1,0)) %>%
  mutate(x = row_number()/nrow(data_test),
         y = (cumsum(adopter_yes)/sum(adopter_yes))/x)
ggplot(data = data_test_lift, aes(x = x, y = y)) +
  geom_line() +
  theme_bw()
```



# 6. Summary
...


Further Work We Can Do: 

1. more explaination about `ovun.sample()` function. How does it work? Does it generate using copy paste or artifically creating or others?
2. more EDA + more visualization (group_by adopter);
3. ROC,AUC for 3 models -> validate the choice of model based on our performance metric; 
4. clustering + feature selections -> modify K-NN and Naive Bayes;
5. weighted distance -> modify K-NN;
6. tree Pruning -> modify Decision Tree;
7. cross validation for 3 models -> validate the robustness of our choosed model;
8. prediction performance visualization