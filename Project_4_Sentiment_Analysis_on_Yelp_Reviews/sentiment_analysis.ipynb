{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adverse-mathematics",
   "metadata": {
    "papermill": {
     "duration": 0.029238,
     "end_time": "2021-05-26T10:49:50.303862",
     "exception": false,
     "start_time": "2021-05-26T10:49:50.274624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    " Here we are going to do binary classification using Yelp Review Sentiment Dataset ([Kaggle Link](https://www.kaggle.com/ilhamfp31/yelp-review-dataset))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "metallic-award",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:49:44.127999Z",
     "iopub.status.busy": "2021-05-26T10:49:44.126462Z",
     "iopub.status.idle": "2021-05-26T10:49:50.181944Z",
     "shell.execute_reply": "2021-05-26T10:49:50.181353Z"
    },
    "papermill": {
     "duration": 6.084801,
     "end_time": "2021-05-26T10:49:50.182101",
     "exception": false,
     "start_time": "2021-05-26T10:49:44.097300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "systematic-florida",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:49:50.240125Z",
     "iopub.status.busy": "2021-05-26T10:49:50.239512Z",
     "iopub.status.idle": "2021-05-26T10:49:50.244710Z",
     "shell.execute_reply": "2021-05-26T10:49:50.244068Z"
    },
    "papermill": {
     "duration": 0.035351,
     "end_time": "2021-05-26T10:49:50.244843",
     "exception": false,
     "start_time": "2021-05-26T10:49:50.209492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disabled-bachelor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:49:50.385646Z",
     "iopub.status.busy": "2021-05-26T10:49:50.385093Z",
     "iopub.status.idle": "2021-05-26T10:49:58.992368Z",
     "shell.execute_reply": "2021-05-26T10:49:58.994383Z"
    },
    "papermill": {
     "duration": 8.661018,
     "end_time": "2021-05-26T10:49:58.994632",
     "exception": false,
     "start_time": "2021-05-26T10:49:50.333614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/yelp-review-dataset/yelp_review_polarity_csv/train.csv', names = ['sentiment', 'text'] )\n",
    "test  = pd.read_csv('../input/yelp-review-dataset/yelp_review_polarity_csv/test.csv',  names = ['sentiment', 'text'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-drill",
   "metadata": {
    "papermill": {
     "duration": 0.047561,
     "end_time": "2021-05-26T10:49:59.126745",
     "exception": false,
     "start_time": "2021-05-26T10:49:59.079184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here, Negative polarity is class 1, and positive class 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neural-milton",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:49:59.234263Z",
     "iopub.status.busy": "2021-05-26T10:49:59.233422Z",
     "iopub.status.idle": "2021-05-26T10:49:59.323003Z",
     "shell.execute_reply": "2021-05-26T10:49:59.323699Z"
    },
    "papermill": {
     "duration": 0.146697,
     "end_time": "2021-05-26T10:49:59.323904",
     "exception": false,
     "start_time": "2021-05-26T10:49:59.177207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  Unfortunately, the frustration of being Dr. Go...\n",
       "1          2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2          1  I don't know what Dr. Goldberg was like before...\n",
       "3          1  I'm writing this review to give you a heads up...\n",
       "4          2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[:15000]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-visit",
   "metadata": {
    "papermill": {
     "duration": 0.082434,
     "end_time": "2021-05-26T10:49:59.473040",
     "exception": false,
     "start_time": "2021-05-26T10:49:59.390606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Before diving into that, lets do little text cleaning/preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "authentic-dispute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:49:59.656626Z",
     "iopub.status.busy": "2021-05-26T10:49:59.655822Z",
     "iopub.status.idle": "2021-05-26T10:49:59.672440Z",
     "shell.execute_reply": "2021-05-26T10:49:59.669375Z"
    },
    "papermill": {
     "duration": 0.111342,
     "end_time": "2021-05-26T10:49:59.672627",
     "exception": false,
     "start_time": "2021-05-26T10:49:59.561285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install bs4\n",
    "# from bs4 import BeautifulSoup\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# import re\n",
    "# lm=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# def ReturnCleanText(text):\n",
    "#         # change the text into lower case.(Note: in case of social media text, it is good to leave them as it is)\n",
    "#         text = text.lower()\n",
    "#         # removing xml tags from tweets\n",
    "#         text =BeautifulSoup(text, 'lxml').get_text()\n",
    "#         # removing URLS \n",
    "#         text =re.sub('https?://[A-Za-z0-9./]+','',text)\n",
    "#         # removing words with \"@\"\n",
    "#         text =re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "#         # removing special characters\n",
    "#         text = re.sub(r\"\\W+|_\", ' ', text)\n",
    "#         # tokenization of sentences\n",
    "#         text = word_tokenize(text)\n",
    "#         # lemmatize the text using WordNetn\n",
    "#         words = [lm.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]   \n",
    "#         return \" \".join(words)\n",
    "    \n",
    "# train['clean_text'] = train['text'].apply(ReturnCleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-variance",
   "metadata": {
    "papermill": {
     "duration": 0.091219,
     "end_time": "2021-05-26T10:49:59.848022",
     "exception": false,
     "start_time": "2021-05-26T10:49:59.756803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LSTM(Long Short Term Memory):\n",
    "> Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turned-louisville",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:49:59.952946Z",
     "iopub.status.busy": "2021-05-26T10:49:59.951591Z",
     "iopub.status.idle": "2021-05-26T10:50:00.010892Z",
     "shell.execute_reply": "2021-05-26T10:50:00.010361Z"
    },
    "papermill": {
     "duration": 0.112974,
     "end_time": "2021-05-26T10:50:00.011018",
     "exception": false,
     "start_time": "2021-05-26T10:49:59.898044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "moderate-schema",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:50:00.069156Z",
     "iopub.status.busy": "2021-05-26T10:50:00.068337Z",
     "iopub.status.idle": "2021-05-26T10:50:03.391223Z",
     "shell.execute_reply": "2021-05-26T10:50:03.390433Z"
    },
    "papermill": {
     "duration": 3.3539,
     "end_time": "2021-05-26T10:50:03.391348",
     "exception": false,
     "start_time": "2021-05-26T10:50:00.037448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '[UNK]' 'the' 'and' 'i' 'a' 'to' 'was' 'of' 'it' 'is' 'in' 'for'\n",
      " 'that' 'my' 'this' 'but' 'with' 'they' 'you']\n",
      "[  15   10   56 1968    6 1913    2    1   13   23   39    1]\n",
      "this is an example to test the [UNK] that we just [UNK]\n"
     ]
    }
   ],
   "source": [
    "# Just for example\n",
    "max_features = 2000\n",
    "Encoder = keras.layers.experimental.preprocessing.TextVectorization( max_tokens = max_features)\n",
    "Encoder.adapt(train['text'].values)\n",
    "\n",
    "vocab = np.array(Encoder.get_vocabulary())\n",
    "print(vocab[:20])\n",
    "\n",
    "example =\"This is an example to test the encoder that we just created!\"\n",
    "print(Encoder(example).numpy())\n",
    "print(\" \".join(vocab[Encoder(example).numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pursuant-milan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:50:03.471787Z",
     "iopub.status.busy": "2021-05-26T10:50:03.461584Z",
     "iopub.status.idle": "2021-05-26T10:50:06.141945Z",
     "shell.execute_reply": "2021-05-26T10:50:06.141394Z"
    },
    "papermill": {
     "duration": 2.724528,
     "end_time": "2021-05-26T10:50:06.142084",
     "exception": false,
     "start_time": "2021-05-26T10:50:03.417556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words = max_features, )\n",
    "tokenizer.fit_on_texts(train['text'].values)\n",
    "X = tokenizer.texts_to_sequences(train['text'].values)\n",
    "X = pad_sequences(X, padding = 'post' ,maxlen=300)\n",
    "Y = pd.get_dummies(train['sentiment']).values\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intermediate-subscription",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:50:06.201416Z",
     "iopub.status.busy": "2021-05-26T10:50:06.200301Z",
     "iopub.status.idle": "2021-05-26T10:50:06.209768Z",
     "shell.execute_reply": "2021-05-26T10:50:06.209147Z"
    },
    "papermill": {
     "duration": 0.040894,
     "end_time": "2021-05-26T10:50:06.209914",
     "exception": false,
     "start_time": "2021-05-26T10:50:06.169020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11250, 300) (11250, 2)\n",
      "(3750, 300) (3750, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-johns",
   "metadata": {
    "papermill": {
     "duration": 0.026001,
     "end_time": "2021-05-26T10:50:06.263970",
     "exception": false,
     "start_time": "2021-05-26T10:50:06.237969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training with Keras default Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-reminder",
   "metadata": {
    "papermill": {
     "duration": 0.025855,
     "end_time": "2021-05-26T10:50:06.316076",
     "exception": false,
     "start_time": "2021-05-26T10:50:06.290221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Keras Embedding Layer: \n",
    "\n",
    "Embedding layers in Keras are trained just like any other layer in your network architecture: they are tuned to minimize the loss function by using the selected optimization method. The major difference with other layers, is that their output is not a mathematical function of the input. Instead the input to the layer is used to index a table with the embedding vectors [1]. However, the underlying automatic differentiation engine has no problem to optimize these vectors to minimize the loss function...\n",
    "\n",
    "So, you cannot say that the Embedding layer in Keras is doing the same as word2vec [2]. Remember that word2vec refers to a very specific network setup which tries to learn an embedding which captures the semantics of words. With Keras's embedding layer, you are just trying to minimize the loss function, so if for instance you are working with a sentiment classification problem, the learned embedding will probably not capture complete word semantics but just their emotional polarity.\n",
    "\n",
    "More Here: \n",
    "1. https://stats.stackexchange.com/questions/324992/how-the-embedding-layer-is-trained-in-keras-embedding-layer\n",
    "2. https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moral-causing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:50:06.375624Z",
     "iopub.status.busy": "2021-05-26T10:50:06.375129Z",
     "iopub.status.idle": "2021-05-26T10:50:06.864637Z",
     "shell.execute_reply": "2021-05-26T10:50:06.863960Z"
    },
    "papermill": {
     "duration": 0.522113,
     "end_time": "2021-05-26T10:50:06.864807",
     "exception": false,
     "start_time": "2021-05-26T10:50:06.342694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          600000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,080,578\n",
      "Trainable params: 1,080,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embid_dim = 300\n",
    "lstm_out = 128\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(max_features, embid_dim, input_length = X.shape[1]))\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seventh-operator",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:50:06.930846Z",
     "iopub.status.busy": "2021-05-26T10:50:06.929993Z",
     "iopub.status.idle": "2021-05-26T10:50:55.440418Z",
     "shell.execute_reply": "2021-05-26T10:50:55.439963Z"
    },
    "papermill": {
     "duration": 48.548299,
     "end_time": "2021-05-26T10:50:55.440581",
     "exception": false,
     "start_time": "2021-05-26T10:50:06.892282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 14s 111ms/step - loss: 0.6354 - accuracy: 0.6088 - val_loss: 0.3236 - val_accuracy: 0.8656\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 8s 96ms/step - loss: 0.3023 - accuracy: 0.8822 - val_loss: 0.2971 - val_accuracy: 0.8813\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 9s 98ms/step - loss: 0.2204 - accuracy: 0.9199 - val_loss: 0.3031 - val_accuracy: 0.8688\n",
      "Epoch 4/5\n",
      "88/88 [==============================] - 8s 96ms/step - loss: 0.1769 - accuracy: 0.9378 - val_loss: 0.3226 - val_accuracy: 0.8589\n",
      "Epoch 5/5\n",
      "88/88 [==============================] - 8s 96ms/step - loss: 0.2048 - accuracy: 0.9249 - val_loss: 0.3627 - val_accuracy: 0.8584\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-kennedy",
   "metadata": {
    "papermill": {
     "duration": 0.137436,
     "end_time": "2021-05-26T10:50:55.716655",
     "exception": false,
     "start_time": "2021-05-26T10:50:55.579219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training with GloVe 300D Embeddings\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "Link: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "little-attempt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:50:56.005314Z",
     "iopub.status.busy": "2021-05-26T10:50:56.004753Z",
     "iopub.status.idle": "2021-05-26T10:54:47.085010Z",
     "shell.execute_reply": "2021-05-26T10:54:47.084260Z"
    },
    "papermill": {
     "duration": 231.231563,
     "end_time": "2021-05-26T10:54:47.085140",
     "exception": false,
     "start_time": "2021-05-26T10:50:55.853577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [03:51, 9504.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embedding_vector = {}\n",
    "f = open('../input/glove840b300dtxt/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "foreign-scoop",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:54:48.472655Z",
     "iopub.status.busy": "2021-05-26T10:54:48.471803Z",
     "iopub.status.idle": "2021-05-26T10:54:48.620796Z",
     "shell.execute_reply": "2021-05-26T10:54:48.620316Z"
    },
    "papermill": {
     "duration": 0.844318,
     "end_time": "2021-05-26T10:54:48.620914",
     "exception": false,
     "start_time": "2021-05-26T10:54:47.776596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37637/37637 [00:00<00:00, 262225.43it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,300))\n",
    "for word,i in tqdm(tokenizer.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dependent-contrary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:54:50.007364Z",
     "iopub.status.busy": "2021-05-26T10:54:50.006784Z",
     "iopub.status.idle": "2021-05-26T10:54:50.012744Z",
     "shell.execute_reply": "2021-05-26T10:54:50.012297Z"
    },
    "papermill": {
     "duration": 0.700992,
     "end_time": "2021-05-26T10:54:50.012859",
     "exception": false,
     "start_time": "2021-05-26T10:54:49.311867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37638, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "worldwide-month",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:54:51.478069Z",
     "iopub.status.busy": "2021-05-26T10:54:51.477102Z",
     "iopub.status.idle": "2021-05-26T10:54:52.173009Z",
     "shell.execute_reply": "2021-05-26T10:54:52.173426Z"
    },
    "papermill": {
     "duration": 1.467787,
     "end_time": "2021-05-26T10:54:52.173634",
     "exception": false,
     "start_time": "2021-05-26T10:54:50.705847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          11291400  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 11,771,978\n",
      "Trainable params: 480,578\n",
      "Non-trainable params: 11,291,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embid_dim = 300\n",
    "lstm_out = 128\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, embid_dim, input_length =X.shape[1], weights = [embedding_matrix] , trainable = False))\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "absolute-programmer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:54:53.590093Z",
     "iopub.status.busy": "2021-05-26T10:54:53.589247Z",
     "iopub.status.idle": "2021-05-26T10:55:32.549654Z",
     "shell.execute_reply": "2021-05-26T10:55:32.549128Z"
    },
    "papermill": {
     "duration": 39.663126,
     "end_time": "2021-05-26T10:55:32.549800",
     "exception": false,
     "start_time": "2021-05-26T10:54:52.886674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 11s 89ms/step - loss: 0.6140 - accuracy: 0.6549 - val_loss: 0.4212 - val_accuracy: 0.8325\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.4253 - accuracy: 0.8231 - val_loss: 0.3975 - val_accuracy: 0.8341\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.3901 - accuracy: 0.8402 - val_loss: 0.4428 - val_accuracy: 0.8240\n",
      "Epoch 4/5\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.3635 - accuracy: 0.8533 - val_loss: 0.3380 - val_accuracy: 0.8528\n",
      "Epoch 5/5\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.3140 - accuracy: 0.8664 - val_loss: 0.3536 - val_accuracy: 0.8568\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-handbook",
   "metadata": {
    "papermill": {
     "duration": 0.86878,
     "end_time": "2021-05-26T10:55:34.380696",
     "exception": false,
     "start_time": "2021-05-26T10:55:33.511916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training with Word2Vec Pre-trained and Trained Embeddings\n",
    "Reference: https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-trick",
   "metadata": {
    "papermill": {
     "duration": 0.834104,
     "end_time": "2021-05-26T10:55:36.015903",
     "exception": false,
     "start_time": "2021-05-26T10:55:35.181799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Word2Vec** is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets.\n",
    "\n",
    "1. Continuous Bag-of-Words Model which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "2. Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
    "\n",
    "Know more here:\n",
    "1. https://jalammar.github.io/illustrated-word2vec/\n",
    "2. https://www.tensorflow.org/tutorials/text/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-workstation",
   "metadata": {
    "papermill": {
     "duration": 0.808834,
     "end_time": "2021-05-26T10:55:37.642355",
     "exception": false,
     "start_time": "2021-05-26T10:55:36.833521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training a Word2Vec Embedding from scratch using Gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arbitrary-intake",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:55:39.307650Z",
     "iopub.status.busy": "2021-05-26T10:55:39.307014Z",
     "iopub.status.idle": "2021-05-26T10:56:03.381008Z",
     "shell.execute_reply": "2021-05-26T10:56:03.381948Z"
    },
    "papermill": {
     "duration": 24.927891,
     "end_time": "2021-05-26T10:56:03.382144",
     "exception": false,
     "start_time": "2021-05-26T10:55:38.454253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:24<00:00, 623.24it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences =[]\n",
    "for t in  tqdm(range(len(train['text']))):\n",
    "    text = nltk.word_tokenize(train['text'][t])\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-adapter",
   "metadata": {
    "papermill": {
     "duration": 0.856093,
     "end_time": "2021-05-26T10:56:05.400090",
     "exception": false,
     "start_time": "2021-05-26T10:56:04.543997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### sg : Either 0 or 1. Default is 0 or CBOW. One must explicitly define Skip-gram by passing 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secure-textbook",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:56:07.136039Z",
     "iopub.status.busy": "2021-05-26T10:56:07.135361Z",
     "iopub.status.idle": "2021-05-26T10:56:07.279711Z",
     "shell.execute_reply": "2021-05-26T10:56:07.279163Z"
    },
    "papermill": {
     "duration": 1.005603,
     "end_time": "2021-05-26T10:56:07.280203",
     "exception": false,
     "start_time": "2021-05-26T10:56:06.274600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-233c7d62079c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences, size=300, min_count=2, sg = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "minimal-stick",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:56:09.016965Z",
     "iopub.status.busy": "2021-05-26T10:56:09.016211Z",
     "iopub.status.idle": "2021-05-26T10:56:09.021381Z",
     "shell.execute_reply": "2021-05-26T10:56:09.020938Z"
    },
    "papermill": {
     "duration": 0.883679,
     "end_time": "2021-05-26T10:56:09.021493",
     "exception": false,
     "start_time": "2021-05-26T10:56:08.137814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-48d0073ca605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vocabulary size: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'embedding_word2vec.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "words = list(w2v_model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "\n",
    "# save model \n",
    "filename = 'embedding_word2vec.txt'\n",
    "w2v_model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "treated-patrick",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:56:11.666029Z",
     "iopub.status.busy": "2021-05-26T10:56:11.601082Z",
     "iopub.status.idle": "2021-05-26T10:56:11.839228Z",
     "shell.execute_reply": "2021-05-26T10:56:11.838820Z"
    },
    "papermill": {
     "duration": 1.959551,
     "end_time": "2021-05-26T10:56:11.839350",
     "exception": false,
     "start_time": "2021-05-26T10:56:09.879799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './embedding_word2vec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a088f3433c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./embedding_word2vec.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './embedding_word2vec.txt'"
     ]
    }
   ],
   "source": [
    "embedding_vector = {}\n",
    "f = open('./embedding_word2vec.txt')\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "protecting-immunology",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:56:13.821402Z",
     "iopub.status.busy": "2021-05-26T10:56:13.820596Z",
     "iopub.status.idle": "2021-05-26T10:56:13.856797Z",
     "shell.execute_reply": "2021-05-26T10:56:13.856327Z"
    },
    "papermill": {
     "duration": 1.122287,
     "end_time": "2021-05-26T10:56:13.856917",
     "exception": false,
     "start_time": "2021-05-26T10:56:12.734630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37637/37637 [00:00<00:00, 1252050.41it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,300))\n",
    "for word,i in tqdm(tokenizer.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "common-tender",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:56:15.585655Z",
     "iopub.status.busy": "2021-05-26T10:56:15.584746Z",
     "iopub.status.idle": "2021-05-26T10:56:16.216180Z",
     "shell.execute_reply": "2021-05-26T10:56:16.215622Z"
    },
    "papermill": {
     "duration": 1.498523,
     "end_time": "2021-05-26T10:56:16.216311",
     "exception": false,
     "start_time": "2021-05-26T10:56:14.717788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 300)          11291400  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 11,771,978\n",
      "Trainable params: 480,578\n",
      "Non-trainable params: 11,291,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embid_dim = 300\n",
    "lstm_out = 128\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, embid_dim, input_length =X.shape[1], weights = [ embedding_matrix] , trainable = False))\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "damaged-english",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T10:56:17.947848Z",
     "iopub.status.busy": "2021-05-26T10:56:17.946985Z",
     "iopub.status.idle": "2021-05-26T11:02:13.268074Z",
     "shell.execute_reply": "2021-05-26T11:02:13.267641Z"
    },
    "papermill": {
     "duration": 356.192194,
     "end_time": "2021-05-26T11:02:13.268197",
     "exception": false,
     "start_time": "2021-05-26T10:56:17.076003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 11s 87ms/step - loss: 0.6929 - accuracy: 0.5204 - val_loss: 0.6933 - val_accuracy: 0.5040\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6925 - accuracy: 0.5178 - val_loss: 0.6935 - val_accuracy: 0.5040\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6927 - accuracy: 0.5150 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.6916 - accuracy: 0.5291 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6926 - accuracy: 0.5177 - val_loss: 0.6936 - val_accuracy: 0.5040\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6923 - accuracy: 0.5210 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.6919 - accuracy: 0.5255 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6920 - accuracy: 0.5248 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6918 - accuracy: 0.5269 - val_loss: 0.6936 - val_accuracy: 0.5040\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6926 - accuracy: 0.5177 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6925 - accuracy: 0.5184 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.6923 - accuracy: 0.5208 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.6920 - accuracy: 0.5240 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6923 - accuracy: 0.5210 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6925 - accuracy: 0.5181 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 7s 82ms/step - loss: 0.6925 - accuracy: 0.5180 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6921 - accuracy: 0.5232 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6922 - accuracy: 0.5223 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6920 - accuracy: 0.5242 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6921 - accuracy: 0.5230 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6927 - accuracy: 0.5164 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6918 - accuracy: 0.5267 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6924 - accuracy: 0.5198 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.6918 - accuracy: 0.5263 - val_loss: 0.6936 - val_accuracy: 0.5040\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6925 - accuracy: 0.5185 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6919 - accuracy: 0.5252 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6922 - accuracy: 0.5220 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6922 - accuracy: 0.5217 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6920 - accuracy: 0.5247 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6921 - accuracy: 0.5227 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6921 - accuracy: 0.5230 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6928 - accuracy: 0.5145 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6932 - accuracy: 0.5105 - val_loss: 0.6940 - val_accuracy: 0.5040\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6926 - accuracy: 0.5175 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6920 - accuracy: 0.5242 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6924 - accuracy: 0.5197 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6918 - accuracy: 0.5276 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6917 - accuracy: 0.5285 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.6926 - accuracy: 0.5173 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 7s 82ms/step - loss: 0.6919 - accuracy: 0.5254 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6920 - accuracy: 0.5240 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6932 - accuracy: 0.5102 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6928 - accuracy: 0.5150 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.6924 - accuracy: 0.5193 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6919 - accuracy: 0.5251 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6922 - accuracy: 0.5214 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.6924 - accuracy: 0.5192 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 0.6922 - accuracy: 0.5218 - val_loss: 0.6938 - val_accuracy: 0.5040\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs = 50, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-baking",
   "metadata": {
    "papermill": {
     "duration": 2.200204,
     "end_time": "2021-05-26T11:02:17.456744",
     "exception": false,
     "start_time": "2021-05-26T11:02:15.256540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Using Pretrained Word2Vec Embedding\n",
    "Reference: https://www.kaggle.com/jaskarancr/word2vec-traditional-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "floating-honey",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T11:02:21.468435Z",
     "iopub.status.busy": "2021-05-26T11:02:21.467932Z",
     "iopub.status.idle": "2021-05-26T11:03:32.934907Z",
     "shell.execute_reply": "2021-05-26T11:03:32.934034Z"
    },
    "papermill": {
     "duration": 73.468938,
     "end_time": "2021-05-26T11:03:32.935057",
     "exception": false,
     "start_time": "2021-05-26T11:02:19.466119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = '../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin'\n",
    "w2v_pretrained_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "billion-boston",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T11:03:37.325333Z",
     "iopub.status.busy": "2021-05-26T11:03:37.324778Z",
     "iopub.status.idle": "2021-05-26T11:03:37.758895Z",
     "shell.execute_reply": "2021-05-26T11:03:37.758445Z"
    },
    "papermill": {
     "duration": 2.45176,
     "end_time": "2021-05-26T11:03:37.759012",
     "exception": false,
     "start_time": "2021-05-26T11:03:35.307252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37637/37637 [00:00<00:00, 87979.85it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,300))\n",
    "for word,i in tqdm(tokenizer.word_index.items()):\n",
    "    try:\n",
    "        embedding_value = w2v_pretrained_model[word]\n",
    "        if embedding_value is not None:\n",
    "            embedding_matrix[i] = embedding_value         \n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),300)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "generous-annex",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T11:03:41.961216Z",
     "iopub.status.busy": "2021-05-26T11:03:41.960332Z",
     "iopub.status.idle": "2021-05-26T11:03:42.618823Z",
     "shell.execute_reply": "2021-05-26T11:03:42.618363Z"
    },
    "papermill": {
     "duration": 2.830918,
     "end_time": "2021-05-26T11:03:42.618950",
     "exception": false,
     "start_time": "2021-05-26T11:03:39.788032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 300)          11291400  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 11,771,978\n",
      "Trainable params: 480,578\n",
      "Non-trainable params: 11,291,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embid_dim = 300\n",
    "lstm_out = 128\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length =300, weights = [embedding_matrix ] , trainable = False))\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stuffed-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T11:03:47.480379Z",
     "iopub.status.busy": "2021-05-26T11:03:47.472753Z",
     "iopub.status.idle": "2021-05-26T11:04:26.107165Z",
     "shell.execute_reply": "2021-05-26T11:04:26.106681Z"
    },
    "papermill": {
     "duration": 40.702688,
     "end_time": "2021-05-26T11:04:26.107304",
     "exception": false,
     "start_time": "2021-05-26T11:03:45.404616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 10s 88ms/step - loss: 0.6205 - accuracy: 0.6361 - val_loss: 0.4557 - val_accuracy: 0.7976\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.4441 - accuracy: 0.8039 - val_loss: 0.3815 - val_accuracy: 0.8408\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.3721 - accuracy: 0.8461 - val_loss: 0.3606 - val_accuracy: 0.8549\n",
      "Epoch 4/5\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 0.3995 - accuracy: 0.8295 - val_loss: 0.3585 - val_accuracy: 0.8539\n",
      "Epoch 5/5\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 0.3558 - accuracy: 0.8563 - val_loss: 0.3552 - val_accuracy: 0.8565\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-bones",
   "metadata": {
    "papermill": {
     "duration": 2.116891,
     "end_time": "2021-05-26T11:04:30.531770",
     "exception": false,
     "start_time": "2021-05-26T11:04:28.414879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using Pretrained word2vec Embedding with Trainable as True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "renewable-logging",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T11:04:34.939103Z",
     "iopub.status.busy": "2021-05-26T11:04:34.938313Z",
     "iopub.status.idle": "2021-05-26T11:04:35.538846Z",
     "shell.execute_reply": "2021-05-26T11:04:35.539256Z"
    },
    "papermill": {
     "duration": 2.883622,
     "end_time": "2021-05-26T11:04:35.539403",
     "exception": false,
     "start_time": "2021-05-26T11:04:32.655781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 300)          11291400  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 11,771,978\n",
      "Trainable params: 11,771,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embid_dim = 300\n",
    "lstm_out = 128\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length =300, weights = [embedding_matrix ],\n",
    "                    trainable = True))\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "painted-prompt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T11:04:40.027689Z",
     "iopub.status.busy": "2021-05-26T11:04:40.027117Z",
     "iopub.status.idle": "2021-05-26T11:06:40.960103Z",
     "shell.execute_reply": "2021-05-26T11:06:40.960475Z"
    },
    "papermill": {
     "duration": 123.297557,
     "end_time": "2021-05-26T11:06:40.960673",
     "exception": false,
     "start_time": "2021-05-26T11:04:37.663116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 27s 270ms/step - loss: 0.6183 - accuracy: 0.6472 - val_loss: 0.3572 - val_accuracy: 0.8581\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 0.4338 - accuracy: 0.8256 - val_loss: 0.3818 - val_accuracy: 0.8315\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 23s 263ms/step - loss: 0.3234 - accuracy: 0.8788 - val_loss: 0.3280 - val_accuracy: 0.8669\n",
      "Epoch 4/5\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 0.2564 - accuracy: 0.9073 - val_loss: 0.3454 - val_accuracy: 0.8768\n",
      "Epoch 5/5\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 0.2098 - accuracy: 0.9209 - val_loss: 0.4229 - val_accuracy: 0.8541\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-sewing",
   "metadata": {
    "papermill": {
     "duration": 2.253582,
     "end_time": "2021-05-26T11:06:49.899443",
     "exception": false,
     "start_time": "2021-05-26T11:06:47.645861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1038.128881,
   "end_time": "2021-05-26T11:06:55.628764",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-26T10:49:37.499883",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
